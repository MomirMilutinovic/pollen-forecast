---
title: "Polen Open Data"
author: "Momir Milutinovic"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

```{r setup, include=FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      include=TRUE,
                      prompt = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      fig.height = 5,
                      fig.width = 7,
                      cache = FALSE)
```

This is a data analysis report about the pollen levels in Belgrade with the explanation of the R code used. Data that will be used for the analysis comes from the [pollen API](http://polen.sepa.gov.rs/api/opendata/schema), which is maintained by the [Environmental Protection Agency](http://sepa.gov.rs).

# Reading and organising data
We will start by reading in the data.

```{r}
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(leaflet))
suppressPackageStartupMessages(library(lubridate))
library(caret)
library(tidyquant)
library(RWeka)

mydata <- read.csv("data/pollenDataMore.csv",
                   header = T,
                   stringsAsFactors = FALSE
                   )
```

Let's what we have here.
```{r}
glimpse(mydata)
```
We don't really need the X and id columns at all.
```{r}
mydata <- mydata[,-c(1, 2)]
glimpse(mydata)
```
That's better!
We will check how many unique records each variable in our data has.
```{r}
(uniq <- unlist(lapply(mydata, function(x) length(unique(x)))))
```
There are 26 stations, of which only two are in Belgrade. We can see that there are more stations than unique geographic coordinate values. This is due to the location of some stations being 0,0 for reasons beyond our control. They are on the second page.
```{r}
print(unique(mydata[, c("location_name", "lat", "long")]))
```
However, none of them are in Belgrade. The two stations in Belgrade are *БЕОГРАД - ЗЕЛЕНО БРДО* and *БЕОГРАД - НОВИ БЕОГРАД* and we will only pay attention to the data from these two stations.

```{r, echo=FALSE}
ggplot(mydata, aes(x=location_name,  colour = as.factor(location_name), fill = as.factor(location_name))) +
  geom_bar()
```

```{r}
  counts <- mydata %>% group_by(location_name, allergen_name)
  summarise(counts, rows = n()) %>% arrange(desc(rows))
```


```{r, echo=FALSE}
mydata <- mydata %>% filter(location_name == c("БЕОГРАД - ЗЕЛЕНО БРДО"))
```

```{r, echo=FALSE}
ggplot(mydata, aes(x = allergen_name, colour = as.factor(allergen_name), fill = as.factor(allergen_name))) +
  geom_bar() +
  xlab("Allergen") +
  ylab("Count")
```

We seem to have almost even numbers of data points for each allegen.

```{r, echo=FALSE}
ggplot(mydata, aes(x = as.Date(date))) +
  geom_histogram(postion = "dodge")
```
The more recent data seems to be more sparse, but we won't be dealing with that here.

```{r}
  mydata %>% 
  filter(allergen_name %in% c("URTICACEAE", "PLATANGO")) %>%
  ggplot(aes(as.Date(date), value, colour=as.factor(allergen_name))) + 
    geom_point() +
    xlab("Date") +
    ylab("Concentration") +
    ggtitle("Concentration of URTICACEAE over time")
```

We will take a look at only the ambrosia data since different allergens might have different distributions as pictured above.

```{r}
mydata <- mydata %>% filter(allergen_name == "URTICACEAE")
```

### Formatting date and time

Separate the year, month and day into separate columns
```{r}
  mydata$date <- as.Date(mydata$date)
  mydata <- mydata %>% mutate(year = lubridate::year(date))  %>%
    mutate(month = lubridate::month(date)) %>% 
    mutate(day = lubridate::day(date))
```

### Adding next day
```{r}
  mydata <- mydata %>% arrange(date)
  mydata <- mydata %>% mutate(next_day = NA)
  # Remove last row that will have NA for next day


  # Add the next day's concentration to the next_day column of there is data for the next day
  # Otherwise set the value to NA
  for(i in 1:(nrow(mydata) - 1)) {
    next_day <- mydata[i + 1,]

    if(as.Date(next_day$date) == (as.Date(mydata[i,]$date) + 1)) {
      mydata[i,]$next_day <- next_day$value
    }
    else {
      mydata[i,]$next_day <- NA
    }

  }

  mydata <- mydata[is.na(mydata$next_day) == FALSE,]
```

# Analysing the readings
```{r, echo=FALSE}
# TODO 
# Compare two allergens and show that they have different patterns.
```

We will take a look at only the ambrosia data since different allergens might have different distributions as pictured above.

```{r, echo=FALSE}

ggplot(mydata, aes(as.Date(date), value)) + 
  geom_point() +
  xlab("Date") +
  ylab("Concentration") +
  ggtitle("Concentration of URTICACEAE over time")
```

We can see some evenly spaced spikes. This is to be expected since there are pollen seasons during a year.

Let's see the distribution of the concentration
```{r, echo=FALSE}
  ggplot(mydata, aes(x = value)) + 
    geom_histogram()
```

As is confirmed by the previous plot, we have lots of small values. This might be a problem for some learning algorithms.


## Data enrichment
Remove dummy variables
```{r}
mydata <- mydata %>% select(-c(allergen_name, localized_name, lat, long, location_name))
```

We'll add weather data.
```{r}
weather <- read.csv("data/wunderground.csv", stringsAsFactors = FALSE)
weather$date <- as.Date(weather$date)
mydata <- mydata %>% inner_join(weather, by=c("date" = "date"))

weather <- read.csv("data/weather.csv", stringsAsFactors = FALSE)
weather <- weather %>% select(c(DATE, PRCP))
weather$DATE <- as.Date(weather$DATE)
mydata <- mydata %>% inner_join(weather, by=c("date" = "DATE"))
mydata$PRCP[is.na(mydata$PRCP)] <- 0

mydata <- mydata %>% select(-X)

```

## Baseline model


### Train-Validation-Test split
```{r}
  train_end <- round(0.6 * nrow(mydata))
  valid_end <- train_end + round(0.2 * nrow(mydata))
  train <- mydata[1:train_end,]
  valid <- mydata[(train_end+1):valid_end, ]
  test <- mydata[(valid_end+1):nrow(mydata),]
  
```


```{r}
mydata <- mydata %>% select(-c(date, Total_percipitation))
algorithms <- c('svmLinear', 'xgbTree', 'rf', 'brnn', 'bayesglm', 'treebag', 'lm')

trainModel <- function(data, method) {
  set.seed(8)
  train(next_day ~ .,
    data=data,
    method=method,
    trControl = trainControl(method = "cv", number = 5),
    metric = 'RMSE',
    preProcess=c("center","scale")
  )

}

baseline.models <- lapply(algorithms, trainModel, data = mydata)
```


Export the split data
```{r}
  #write.csv(train, "data/train.csv")
  #write.csv(valid, "data/valid.csv")
  #write.csv(test, "data/test.csv")
```

## Feature engineering

Let's add columns that depict the change in temperature, pressure, etc.

```{r}
  mydata <- mydata %>% mutate(delta_t = (Max_temp - Min_temp))
  mydata <- mydata %>% mutate(delta_dew = (Max_dew - Min_dew))
  mydata <- mydata %>% mutate(delta_humid = Max__humidity - Min__humidity)
  mydata <- mydata %>% mutate(delta_pressure = Max_pressure - Min_pressure)
  mydata <- mydata %>% mutate(delta_wind = Max_wind - Min_wind)

```


```{r}
delta.models <- lapply(algorithms, trainModel, data = mydata)
```

### Day encoding
We will try to use only the information of what day it is in a year. We are going to use the number of the day in the year.

```{r}
mydata <- mydata %>% mutate(day_serial = yday(make_date(year = year, month = month, day = day)))
mydata <- mydata %>% select(-c(year,month,day))
```


### Retrain

```{r}
algorithms <- c('svmLinear', 'xgbTree', 'rf', 'brnn', 'bayesglm', 'treebag', 'lm')

encodedDay.models <- lapply(algorithms, trainModel, data = mydata)
```
